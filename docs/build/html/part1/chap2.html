

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 2 Intelligent Agents &mdash; ArtificialIntelligence  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 2 Problem-solving" href="../part2/index2.html" />
    <link rel="prev" title="Chapter 1 Introduction" href="chap1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> ArtificialIntelligence
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index1.html">Part 1 Artificial Intelligence</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap1.html">Chapter 1 Introduction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 2 Intelligent Agents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#agents-and-environments">2.1 Agents and Environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#good-behavior-the-concept-of-rationality">2.2 Good Behavior: The Concept of Rationality</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#performance-measures">2.2.1 Performance measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rationality">2.2.2 Rationality</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-nature-of-environments">2.3 The Nature of Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#specifying-the-task-environment">2.3.1 Specifying the task environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#properties-of-task-environments">2.3.2 Properties of task environments</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-structure-of-agents">2.4 The Structure of Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#agent-programs">2.4.1 Agent programs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#simple-reflex-agents">2.4.2 Simple reflex Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-based-reflex-agents">2.4.3 Model-based reflex agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="#utility-based-agents">2.4.5 Utility-based agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="#learning-agents">2.4.6 Learning agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-the-components-of-agent-programs-work">2.4.7 How the components of agent programs work</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../part2/index2.html">Part 2 Problem-solving</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ArtificialIntelligence</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index1.html">Part 1 Artificial Intelligence</a> &raquo;</li>
        
      <li>Chapter 2 Intelligent Agents</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/part1/chap2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\cl}{\mathcal}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\lv}{\lVert}
\newcommand{\ol}{\overline}
\newcommand{\ra}{\rightarrow}
\newcommand{\rv}{\rVert}
\newcommand{\seq}{\subseteq}
\newcommand{\vds}{\vdots}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\1}{\boldsymbol{1}}
\newcommand{\a}{\boldsymbol{\mathrm{a}}}
\newcommand{\b}{\boldsymbol{\mathrm{b}}}
\newcommand{\c}{\boldsymbol{\mathrm{c}}}
\newcommand{\e}{\boldsymbol{\mathrm{e}}}
\newcommand{\f}{\boldsymbol{\mathrm{f}}}
\newcommand{\g}{\boldsymbol{\mathrm{g}}}
\newcommand{\i}{\boldsymbol{\mathrm{i}}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\n}{\boldsymbol{\mathrm{n}}}
\newcommand{\p}{\boldsymbol{\mathrm{p}}}
\newcommand{\q}{\boldsymbol{\mathrm{q}}}
\newcommand{\r}{\boldsymbol{\mathrm{r}}}
\newcommand{\u}{\boldsymbol{\mathrm{u}}}
\newcommand{\v}{\boldsymbol{\mathrm{v}}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{\mathrm{x}}}
\newcommand{\y}{\boldsymbol{\mathrm{y}}}\\\newcommand{\A}{\boldsymbol{\mathrm{A}}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\K}{\boldsymbol{\mathrm{K}}}
\newcommand{\N}{\boldsymbol{\mathrm{N}}}
\newcommand{\P}{\boldsymbol{\mathrm{P}}}
\newcommand{\S}{\boldsymbol{\mathrm{S}}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\W}{\boldsymbol{\mathrm{W}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}\\\newcommand{\R}{\mathbb{R}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\boldsymbol{\mathrm{\Lambda}}}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\boldsymbol{\mathrm{\Sigma}}}
\newcommand{\th}{\theta}\\\newcommand{\mmu}{\boldsymbol{\mu}}\\\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-2-intelligent-agents">
<h1>Chapter 2 Intelligent Agents<a class="headerlink" href="#chapter-2-intelligent-agents" title="Permalink to this headline">¶</a></h1>
<div class="section" id="agents-and-environments">
<h2>2.1 Agents and Environments<a class="headerlink" href="#agents-and-environments" title="Permalink to this headline">¶</a></h2>
<p>An <strong>agent</strong> is anything that can be viewed as perceiving its <strong>environment</strong>
through <strong>sensors</strong> and acting upon that environment through <strong>actuators</strong>.</p>
<img alt="../_images/Fig2.1.png" src="../_images/Fig2.1.png" />
<p>We use the term <strong>percept</strong> to refer to the content an agent’s sensors are perceiving.
An agent’s <strong>percept sequence</strong> is the complete history of everything the agent has ever perceived.
In general, <em>an agent’s choice of action at any given instant can depend on its</em>
<em>built-in knowledge and on the entire percept sequence observed to date, but</em>
<em>not on anything it hasn’t perceived</em>.
Mathematically speaking, we say that an agent’s behavior is described by the
<strong>agent function</strong> that maps any given percept sequence to an action.</p>
<p><em>Internally</em>, the agent function for an artificial agent will be implemented by an <strong>agent program</strong>.
It is important to keep these two ideas distinct.
The agent function is an abstract mathematical description; the agent program is
a concrete implementation, running within some physical system.</p>
</div>
<div class="section" id="good-behavior-the-concept-of-rationality">
<h2>2.2 Good Behavior: The Concept of Rationality<a class="headerlink" href="#good-behavior-the-concept-of-rationality" title="Permalink to this headline">¶</a></h2>
<p>A <strong>rational agent</strong> is one that does the right thing.</p>
<div class="section" id="performance-measures">
<h3>2.2.1 Performance measures<a class="headerlink" href="#performance-measures" title="Permalink to this headline">¶</a></h3>
<p>Moral philosophy has developed several different notions of the “right thing,”
but AI has generally stuck to one notion called <strong>consequentialism</strong>: we
evaluate an agent’s behavior by its consequences.</p>
<p>This notion of desirability is captured by a <strong>performance measure</strong> that
evaluates any given sequence of environment states.</p>
</div>
<div class="section" id="rationality">
<h3>2.2.2 Rationality<a class="headerlink" href="#rationality" title="Permalink to this headline">¶</a></h3>
<p>What is rational at any given time depends on four things:</p>
<ul class="simple">
<li><p>The performance measure that defines the criterion of success.</p></li>
<li><p>The agent’s prior knowledge of the environment.</p></li>
<li><p>The actions that the agent can perform.</p></li>
<li><p>The agent’s percept sequence to date.</p></li>
</ul>
<p><strong>Definition of a rational agent</strong>: <em>For each possible percept sequence, a</em>
<em>rational agent should select an action that is expected to maximize its</em>
<em>performance measure, given the evidence provided by the percept sequence and</em>
<em>whatever built-in knowledge the agent has</em>.</p>
<p>2.2.3 Omniscience, learning, and autonomy</p>
<p>We need to be careful to distinguish between rationality and <strong>omniscience</strong>.
An omniscient agent knows the <em>actual</em> outcome of its actions and can act
accordingly; but omniscience is impossible in reality.</p>
<p>Rationality maximizes <em>expected</em> performance, while perfection maximizes <em>actual</em> performance.</p>
<p>Doing actions <em>in order to modify future percepts</em>—sometimes called <strong>information gathering</strong>.
Our definition requires a rational agent not only to gather information but also
to <strong>learn</strong> as much as possible from what it perceives.</p>
<p>To the extent that an agent relies on the prior knowledge of its designer rather
than on its own percepts and learning processes, we say that the agent lacks
<strong>autonomy</strong>.
A rational agent should be autonomous—it should learn what it can to compensate
for partial or incorrect prior knowledge.</p>
</div>
</div>
<div class="section" id="the-nature-of-environments">
<h2>2.3 The Nature of Environments<a class="headerlink" href="#the-nature-of-environments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="specifying-the-task-environment">
<h3>2.3.1 Specifying the task environment<a class="headerlink" href="#specifying-the-task-environment" title="Permalink to this headline">¶</a></h3>
<p>The <strong>task environment</strong> is composed of <strong>PEAS</strong> (Performance, Environment, Actuators, Sensors).</p>
</div>
<div class="section" id="properties-of-task-environments">
<h3>2.3.2 Properties of task environments<a class="headerlink" href="#properties-of-task-environments" title="Permalink to this headline">¶</a></h3>
<p><strong>FULLY OBSERVABLE VS. PARTIALLY OBSERVABLE</strong>: If an agent’s sensors give it
access to the complete state of the environment at each point in time, then we
say that the task environment is fully observable.
An environment might be partially observable because of noisy and inaccurate
sensors or because parts of the state are simply missing from the sensor data.
If the agent has no sensors at all then the environment is <strong>unobservable</strong>.</p>
<p><strong>SINGLE-AGENT VS. MULTIAGENT</strong>: The agent-design problems in multiagent
environments are often quite different from those in single-agent environments.</p>
<p><strong>DETERMINISTIC VS. NONDETERMINISTIC</strong>: If the next state of the environment is
completely determined by the current state and the action executed by the
agent(s), then we say the environment is deterministic; otherwise, it is
nondeterministic.</p>
<p><strong>EPISODIC VS. SEQUENTIAL</strong>: In an episodic task environment, the agent’s
experience is divided into atomic episodes.
Crucially, the next episode does not depend on the actions taken in previous episodes.
In sequential environments, on the other hand, the current decision could affect all future decisions.</p>
<p><strong>STATIC VS. DYNAMIC</strong>: If the environment can change while an agent is
deliberating, then we say the environment is dynamic for that agent; otherwise,
it is static.
If the environment itself does not change with the passage of time but the
agent’s performance score does, then we say the environment is <strong>semidynamic</strong>.</p>
<p><strong>DISCRETE VS. CONTINUOUS</strong>: The discrete/continuous distinction applies to the
<em>state</em> of the environment, to the way <em>time</em> is handled, and to the <em>percepts</em>
and <em>actions</em> of the agent.</p>
<p><strong>KNOWN VS. UNKNOWN</strong>: In a known environment, the outcomes (or outcome
probabilities if the environment is nondeterministic) for all actions are given.
Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions.</p>
<p>The performance measure itself may be unknown, either because the designer is
not sure how to write it down correctly or because the ultimate user—whose
preferences matter—is not known.
The hardest case is <em>partially observable</em>, <em>multiagent</em>, <em>nondeterministic</em>,
<em>sequential</em>, <em>dynamic</em>, <em>continuous</em>, and <em>unknown</em>.</p>
</div>
</div>
<div class="section" id="the-structure-of-agents">
<h2>2.4 The Structure of Agents<a class="headerlink" href="#the-structure-of-agents" title="Permalink to this headline">¶</a></h2>
<p>The job of AI is to design an <strong>agent program</strong> that implements the agent function—the mapping from percepts to actions.
We assume this program will run on some sort of computing device with physical
sensors and actuators—we call this the <strong>agent architecture</strong>:</p>
<blockquote>
<div><p><em>agent</em> = <em>architecture</em> + <em>program</em>.</p>
</div></blockquote>
<div class="section" id="agent-programs">
<h3>2.4.1 Agent programs<a class="headerlink" href="#agent-programs" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Fig2.7.png" src="../_images/Fig2.7.png" />
<p>The daunting size of these tables means that (a) no physical agent in this
universe will have the space to store the table; (b) the designer would not have
time to create the table; and (c) no agent could ever learn all the right table
entries from its experience.</p>
<p><em>The key challenge for AI is to find out how to write programs that, to the</em>
<em>extent possible, produce rational behavior from a smallish program rather than</em>
<em>from a vast table</em>.</p>
</div>
<div class="section" id="simple-reflex-agents">
<h3>2.4.2 Simple reflex Agents<a class="headerlink" href="#simple-reflex-agents" title="Permalink to this headline">¶</a></h3>
<p>The simplest kind of agent is the <strong>simple reflex agent</strong>.
These agents select actions on the basis of the <em>current</em> percept, ignoring the rest of the percept history.</p>
<img alt="../_images/Fig2.8.png" src="../_images/Fig2.8.png" />
<p>A more general and flexible approach is first to build a general-purpose
interpreter for condition–action rules and then to create rule sets for specific
task environments.</p>
<img alt="../_images/Fig2.9.png" src="../_images/Fig2.9.png" />
<img alt="../_images/Fig2.10.png" src="../_images/Fig2.10.png" />
<p>The agent in Figure 2.10 will work <em>only if the correct decision can be made on</em>
<em>the basis of just the current percept—that is, only if the environment is</em>
<em>fully observable</em>.
Even a little bit of unobservability can cause serious trouble.
Infinite loops are often unavoidable for simple reflex agents operating in
partially observable environments.
Escape from infinite loops is possible if the agent can <strong>randomize</strong> its actions.</p>
</div>
<div class="section" id="model-based-reflex-agents">
<h3>2.4.3 Model-based reflex agents<a class="headerlink" href="#model-based-reflex-agents" title="Permalink to this headline">¶</a></h3>
<p>The most effective way to handle partial observability is for the agent to
<em>keep track of the part of the world it can’t see now</em>.
That is, the agent should maintain some sort of <strong>internal state</strong> that depends
on the percept history and thereby reflects at least some of the unobserved
aspects of the current state.</p>
<p>Updating this internal state information as time goes by requires two kinds of
knowledge to be encoded in the agent program in some form.
First, we need some information about how the world changes over time, which can
be divided roughly into two parts: the effects of the agent’s actions and how
the world evolves independently of the agent.
This knowledge about “how the world works”—whether implemented in simple Boolean
circuits or in complete scientific theories—is called a <strong>transition model</strong> of
the world.
Second, we need some information about how the state of the world is reflected in the agent’s percepts.
This kind of knowledge is called a <strong>sensor model</strong>.</p>
<p>An agent that uses such models is called a <strong>model-based agent</strong>.</p>
<img alt="../_images/Fig2.11.png" src="../_images/Fig2.11.png" />
<img alt="../_images/Fig2.12.png" src="../_images/Fig2.12.png" />
<p>2.4.4 Goal-based agents</p>
<p>As well as a current state description, the agent needs some sort of goal
information that describes situations that are desirable.
The agent program can combine this with the model (the same information as was
used in the model-based reflex agent) to choose actions that achieve the goal.</p>
<img alt="../_images/Fig2.13.png" src="../_images/Fig2.13.png" />
<p><strong>Search</strong> and <strong>planning</strong> are the subfields of AI devoted to finding action sequences that achieve the agent’s goals.</p>
</div>
<div class="section" id="utility-based-agents">
<h3>2.4.5 Utility-based agents<a class="headerlink" href="#utility-based-agents" title="Permalink to this headline">¶</a></h3>
<p>An agent’s <strong>utility function</strong> is essentially an internalization of the performance measure.
Provided that the internal utility function and the external performance measure
are in agreement, an agent that chooses actions to maximize its utility will be
rational according to the external performance measure.</p>
<p>A rational utility-based agent chooses the action that maximizes the
<strong>expected utility</strong> of the action outcomes—that is, the utility the agent
expects to derive, on average, given the probabilities and utilities of each
outcome.</p>
<img alt="../_images/Fig2.14.png" src="../_images/Fig2.14.png" />
<p>A <strong>model-free agent</strong> can learn what action is best in a particular situation
without ever learning exactly how that action changes the environment.</p>
</div>
<div class="section" id="learning-agents">
<h3>2.4.6 Learning agents<a class="headerlink" href="#learning-agents" title="Permalink to this headline">¶</a></h3>
<p>A learning agent can be divided into four conceptual components.
The most important distinction is between the <strong>learning element</strong>, which is
responsible for making improvements, and the <strong>performance element</strong>, which is
responsible for selecting external actions.
The performance element is what we have previously considered to be the entire
agent: it takes in percepts and decides on actions.
The learning element uses feedback from the <strong>critic</strong> on how the agent is doing
and determines how the performance element should be modified to do better in
the future.</p>
<img alt="../_images/Fig2.15.png" src="../_images/Fig2.15.png" />
<p>The design of the learning element depends very much on the design of the performance element.
Given a design for the performance element, learning mechanisms can be constructed to improve every part of the agent.</p>
<p>The critic tells the learning element how well the agent is doing with respect to a fixed performance standard.
The critic is necessary because the percepts themselves provide no indication of the agent’s success.
It is important that the performance standard be fixed.</p>
<p>The last component of the learning agent is the <strong>problem generator</strong>.
It is responsible for suggesting actions that will lead to new and informative experiences.</p>
<p>Improving the model components of a model-based agent so that they conform
better with reality is almost always a good idea, regardless of the external
performance standard.
In a sense, the performance standard distinguishes part of the incoming percept
as a <strong>reward</strong> (or <strong>penalty</strong>) that provides direct feedback on the quality of
the agent’s behavior.
More generally, <em>human choices</em> can provide information about human preferences.</p>
<p>Learning in intelligent agents can be summarized as a process of modification of
each component of the agent to bring the components into closer agreement with
the available feedback information, thereby improving the overall performance of
the agent.</p>
</div>
<div class="section" id="how-the-components-of-agent-programs-work">
<h3>2.4.7 How the components of agent programs work<a class="headerlink" href="#how-the-components-of-agent-programs-work" title="Permalink to this headline">¶</a></h3>
<p>Roughly speaking, we can place the representations along an axis of increasing
complexity and expressive power—atomic, factored, and structured.</p>
<img alt="../_images/Fig2.16.png" src="../_images/Fig2.16.png" />
<p>In an <strong>atomic representation</strong> each state of the world is indivisible—it has no internal structure.
The standard algorithms underlying search and game-playing, hidden Markov
models, and Markov decision processes all work with atomic representations.</p>
<p>A <strong>factored representation</strong> splits up each state into a fixed set of
<strong>variables</strong> or <strong>attributes</strong>, each of which can have a <strong>value</strong>.
Many important areas of AI are based on factored representations, including
constraint satisfaction algorithms, propositional logic, planning, Bayesian
networks, and various machine learning algorithms.</p>
<p>In a <strong>Structured representation</strong>, objects and their various and varying relationships can be described explicitly.
Structured representations underlie relational databases and first-order logic,
first-order probability models, and much of natural language understanding.</p>
<p>The axis along which atomic, factored, and structured representations lie is the
axis of increasing <strong>expressiveness</strong>.
Roughly speaking, a more expressive representation can capture, at least as
concisely, everything a less expressive one can capture, plus some more.</p>
<p>Another axis for representation involves the mapping of concepts to locations in
physical memory, whether in a computer or in a brain.
If there is a one-to-one mapping between concepts and memory locations, we call that a <strong>localist representation</strong>.
On the other hand, if the representation of a concept is spread over many memory
locations, and each memory location is employed as part of the representation of
multiple different concepts, we call that a <strong>distributed representation</strong>.
Distributed representations are more robust against noise and information loss.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../part2/index2.html" class="btn btn-neutral float-right" title="Part 2 Problem-solving" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap1.html" class="btn btn-neutral float-left" title="Chapter 1 Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>